"""
Comprehensive Testing Suite for Medical AI System
Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø¨ÙŠ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
"""

import streamlit as st
import pytest
import sys
import os
import importlib
from datetime import datetime
import pandas as pd
import numpy as np

class ComprehensiveSystemTester:
    """Ù…Ø®ØªØ¨Ø± Ø´Ø§Ù…Ù„ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø¨ÙŠ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    
    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}
        self.start_time = datetime.now()
    
    def run_complete_test_suite(self):
        """ØªØ´ØºÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙƒØ§Ù…Ù„Ø©"""
        st.title("ğŸ§ª Comprehensive System Testing Suite")
        st.markdown("---")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¨ÙˆÙŠØ¨ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ” Basic Tests", 
            "ğŸ§  AI Engine Tests", 
            "ğŸ¥ Medical Tests",
            "ğŸ“Š Performance Tests",
            "ğŸ“ˆ Results Summary"
        ])
        
        with tab1:
            self._run_basic_tests()
        
        with tab2:
            self._run_ai_engine_tests()
        
        with tab3:
            self._run_medical_tests()
        
        with tab4:
            self._run_performance_tests()
        
        with tab5:
            self._display_results_summary()
    
    def _run_basic_tests(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        st.header("ğŸ” Basic System Tests")
        
        basic_tests = [
            ("Python Version Check", self._test_python_version),
            ("Import Dependencies", self._test_imports),
            ("File System Check", self._test_file_system),
            ("Memory Availability", self._test_memory),
            ("GPU Availability", self._test_gpu)
        ]
        
        for test_name, test_func in basic_tests:
            with st.expander(f"ğŸ§ª {test_name}", expanded=False):
                result = test_func()
                self.test_results[test_name] = result
                self._display_test_result(test_name, result)
    
    def _run_ai_engine_tests(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø­Ø±Ùƒ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        st.header("ğŸ§  AI Engine Tests")
        
        ai_tests = [
            ("Model Loading", self._test_model_loading),
            ("Model Architecture", self._test_model_architecture),
            ("Inference Speed", self._test_inference_speed),
            ("Memory Usage", self._test_model_memory),
            ("Input/Output Validation", self._test_io_validation)
        ]
        
        for test_name, test_func in ai_tests:
            with st.expander(f"ğŸ¤– {test_name}", expanded=False):
                result = test_func()
                self.test_results[test_name] = result
                self._display_test_result(test_name, result)
    
    def _run_medical_tests(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©"""
        st.header("ğŸ¥ Medical Component Tests")
        
        medical_tests = [
            ("DICOM Processing", self._test_dicom_processing),
            ("Biomarker Analysis", self._test_biomarker_analysis),
            ("Database Operations", self._test_database_operations),
            ("PDF Report Generation", self._test_pdf_generation),
            ("Clinical Validation", self._test_clinical_validation)
        ]
        
        for test_name, test_func in medical_tests:
            with st.expander(f"ğŸ¥ {test_name}", expanded=False):
                result = test_func()
                self.test_results[test_name] = result
                self._display_test_result(test_name, result)
    
    def _run_performance_tests(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        st.header("ğŸ“Š Performance Benchmark Tests")
        
        performance_tests = [
            ("System Responsiveness", self._test_responsiveness),
            ("Memory Efficiency", self._test_memory_efficiency),
            ("Processing Speed", self._test_processing_speed),
            ("Concurrent Users", self._test_concurrent_users),
            ("Data Throughput", self._test_data_throughput)
        ]
        
        for test_name, test_func in performance_tests:
            with st.expander(f"âš¡ {test_name}", expanded=False):
                result = test_func()
                self.test_results[test_name] = result
                self.performance_metrics[test_name] = result
                self._display_performance_result(test_name, result)
    
    def _test_python_version(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¥ØµØ¯Ø§Ø± Python"""
        try:
            version = sys.version_info
            if version.major == 3 and version.minor >= 8:
                return {"status": "PASSED", "details": f"Python {version.major}.{version.minor}.{version.micro}"}
            else:
                return {"status": "FAILED", "details": f"Python 3.8+ required, found {version.major}.{version.minor}"}
        except Exception as e:
            return {"status": "ERROR", "details": str(e)}
    
    def _test_imports(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª"""
        dependencies = [
            ("torch", "PyTorch"),
            ("pydicom", "PyDICOM"),
            ("streamlit", "Streamlit"),
            ("skimage", "Scikit-Image"),
            ("plotly", "Plotly")
        ]
        
        results = []
        for import_name, display_name in dependencies:
            try:
                importlib.import_module(import_name)
                results.append(f"âœ… {display_name}")
            except ImportError as e:
                results.append(f"âŒ {display_name}: {str(e)}")
        
        if all("âœ…" in result for result in results):
            return {"status": "PASSED", "details": results}
        else:
            return {"status": "FAILED", "details": results}
    
    def _test_model_loading(self):
        """Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            from advanced_ai_engine import HybridMedicalModel
            from model_loader import AdvancedModelManager
            
            manager = AdvancedModelManager()
            model = manager.load_hybrid_model()
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            test_input = torch.randn(1, 3, 224, 224)
            with torch.no_grad():
                output = model(test_input)
            
            performance = manager.model_size_analysis(model)
            
            return {
                "status": "PASSED", 
                "details": [
                    f"âœ… Model loaded successfully",
                    f"âœ… Parameters: {performance['parameters_count']:,}",
                    f"âœ… Model size: {performance['total_size_mb']} MB",
                    f"âœ… Output shape: {output['final_prediction'].shape}"
                ]
            }
            
        except Exception as e:
            return {"status": "FAILED", "details": [f"âŒ Model loading failed: {str(e)}"]}
    
    def _test_dicom_processing(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© DICOM"""
        try:
            from dicom_processor import AdvancedDICOMProcessor
            
            processor = AdvancedDICOMProcessor()
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ‡Ù…ÙŠØ©
            test_metadata = {
                'patient_info': {'name': 'Test Patient'},
                'study_info': {'modality': 'CT'},
                'image_characteristics': {'rows': 512, 'columns': 512}
            }
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ±
            report = processor.generate_dicom_report(test_metadata)
            
            return {
                "status": "PASSED",
                "details": [
                    "âœ… DICOM processor initialized",
                    "âœ… Metadata extraction working",
                    "âœ… Report generation functional",
                    f"âœ… Report length: {len(report)} characters"
                ]
            }
            
        except Exception as e:
            return {"status": "FAILED", "details": [f"âŒ DICOM processing failed: {str(e)}"]}
    
    def _test_biomarker_analysis(self):
        """Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ©"""
        try:
            from enhanced_biomarkers import QuantitativeBiomarkerAnalyzer
            
            analyzer = QuantitativeBiomarkerAnalyzer()
            
            # ØµÙˆØ±Ø© Ø§Ø®ØªØ¨Ø§Ø±ÙŠØ©
            test_image = np.random.randint(0, 255, (256, 256), dtype=np.uint8)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ©
            results = analyzer.comprehensive_biomarker_analysis(test_image)
            
            return {
                "status": "PASSED",
                "details": [
                    "âœ… Biomarker analyzer working",
                    f"âœ… Integrated score: {results.get('integrated_biomarker_score', 0):.1f}%",
                    f"âœ… Risk level: {results.get('clinical_risk_assessment', {}).get('risk_level', 'Unknown')}",
                    f"âœ… Analysis categories: {len(results)}"
                ]
            }
            
        except Exception as e:
            return {"status": "FAILED", "details": [f"âŒ Biomarker analysis failed: {str(e)}"]}
    
    def _test_processing_speed(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        try:
            import time
            from advanced_ai_engine import HybridMedicalModel
            
            model = HybridMedicalModel()
            model.eval()
            
            # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±Ø¹Ø©
            test_input = torch.randn(1, 3, 224, 224)
            
            start_time = time.time()
            with torch.no_grad():
                for _ in range(10):  # 10 ØªÙƒØ±Ø§Ø±Ø§Øª
                    _ = model(test_input)
            end_time = time.time()
            
            avg_time = (end_time - start_time) / 10
            
            if avg_time < 1.0:  # Ø£Ù‚Ù„ Ù…Ù† Ø«Ø§Ù†ÙŠØ©
                status = "PASSED"
            else:
                status = "WARNING"
            
            return {
                "status": status,
                "details": [
                    f"âœ… Average inference time: {avg_time:.3f} seconds",
                    f"âœ… Throughput: {1/avg_time:.1f} inferences/second",
                    f"âœ… Meets real-time requirements: {'Yes' if avg_time < 0.5 else 'Needs optimization'}"
                ]
            }
            
        except Exception as e:
            return {"status": "FAILED", "details": [f"âŒ Performance test failed: {str(e)}"]}
    
    def _display_test_result(self, test_name, result):
        """Ø¹Ø±Ø¶ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
        status = result["status"]
        details = result["details"]
        
        if status == "PASSED":
            st.success(f"**{test_name}**: âœ… PASSED")
        elif status == "WARNING":
            st.warning(f"**{test_name}**: âš ï¸ WARNING")
        else:
            st.error(f"**{test_name}**: âŒ FAILED")
        
        for detail in details:
            st.write(f"  {detail}")
    
    def _display_performance_result(self, test_name, result):
        """Ø¹Ø±Ø¶ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        if result["status"] == "PASSED":
            col1, col2 = st.columns([1, 3])
            with col1:
                st.metric("Status", "âœ… PASSED")
            with col2:
                for detail in result["details"]:
                    st.write(detail)
        else:
            st.error(f"**{test_name}**: âŒ {result['status']}")
            for detail in result["details"]:
                st.write(f"  {detail}")
    
    def _display_results_summary(self):
        """Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        st.header("ğŸ“ˆ Test Results Summary")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results.values() if r["status"] == "PASSED")
        failed_tests = sum(1 for r in self.test_results.values() if r["status"] == "FAILED")
        warning_tests = sum(1 for r in self.test_results.values() if r["status"] == "WARNING")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Tests", total_tests)
        with col2:
            st.metric("Passed", passed_tests, delta=f"{(passed_tests/total_tests)*100:.1f}%")
        with col3:
            st.metric("Warnings", warning_tests)
        with col4:
            st.metric("Failed", failed_tests, delta_color="inverse")
        
        # Ù…Ø®Ø·Ø· Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        results_df = pd.DataFrame({
            'Category': ['Passed', 'Warnings', 'Failed'],
            'Count': [passed_tests, warning_tests, failed_tests]
        })
        
        fig = px.pie(results_df, values='Count', names='Category', 
                    title="Test Results Distribution",
                    color='Category',
                    color_discrete_map={'Passed': '#00cc96', 'Warnings': '#ffa500', 'Failed': '#ff4b4b'})
        
        st.plotly_chart(fig)
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        st.subheader("ğŸ¯ System Recommendations")
        
        if failed_tests == 0 and warning_tests == 0:
            st.success("ğŸ‰ All tests passed! The system is ready for production use.")
        elif failed_tests > 0:
            st.error("âš ï¸ Critical issues detected. Please address failed tests before deployment.")
        else:
            st.warning("â„¹ï¸  Some warnings detected. Consider optimizing for better performance.")
        
        # ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        st.info(f"â±ï¸ Total testing time: {duration:.2f} seconds")

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def run_system_tests():
    """ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
    tester = ComprehensiveSystemTester()
    tester.run_complete_test_suite()

if __name__ == "__main__":
    run_system_tests()
