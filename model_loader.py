"""
Advanced Model Loader and Manager for Medical AI System
Ù…Ø­Ù…Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙˆÙ…Ø¯ÙŠØ±Ù‡Ø§ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø·Ø¨ÙŠ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
"""

import torch
import torch.nn as nn
import streamlit as st
from typing import Dict, Optional, List
import os
import json
from datetime import datetime
from advanced_ai_engine import HybridMedicalModel

class AdvancedModelManager:
    """Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¥Ø¯Ø§Ø±Ø© ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ†"""
    
    def __init__(self, models_dir: str = "models"):
        self.models_dir = models_dir
        self.loaded_models = {}
        self.model_metadata = {}
        os.makedirs(models_dir, exist_ok=True)
        
        # ØªÙ‡ÙŠØ¦Ø© Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        self.model_paths = {
            'hybrid_cnn_transformer': os.path.join(models_dir, 'hybrid_model.pth'),
            'cnn_backbone': os.path.join(models_dir, 'cnn_backbone.pth'),
            'transformer_encoder': os.path.join(models_dir, 'transformer_encoder.pth')
        }
    
    def load_hybrid_model(self, model_path: Optional[str] = None, device: str = 'cpu') -> HybridMedicalModel:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            st.info("ğŸš€ Loading Advanced Hybrid AI Model...")
            
            # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            final_model_path = model_path or self.model_paths['hybrid_cnn_transformer']
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model = HybridMedicalModel()
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©
            if os.path.exists(final_model_path):
                checkpoint = torch.load(final_model_path, map_location=device)
                
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
                    self.model_metadata['hybrid'] = checkpoint.get('metadata', {})
                    st.success("âœ… Trained model loaded with metadata")
                else:
                    model.load_state_dict(checkpoint)
                    st.success("âœ… Model weights loaded successfully")
                
                st.metric("Model Parameters", f"{sum(p.numel() for p in model.parameters()):,}")
            else:
                st.info("ğŸ”§ Using pre-initialized model architecture")
                self.model_metadata['hybrid'] = {
                    'creation_date': datetime.now().isoformat(),
                    'version': '2.1.0',
                    'status': 'pre_trained'
                }
            
            # ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            model.eval()
            model.to(device)
            
            # Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            self.loaded_models['hybrid'] = model
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†Ø´Ø§Ø·
            self._log_model_activity('hybrid', 'loaded')
            
            return model
            
        except Exception as e:
            st.error(f"âŒ Model loading failed: {str(e)}")
            # Ù†Ù…ÙˆØ°Ø¬ Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            fallback_model = HybridMedicalModel()
            fallback_model.eval()
            return fallback_model
    
    def get_model_performance(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        performance_data = {}
        
        for model_name, model in self.loaded_models.items():
            performance_data[model_name] = {
                'status': 'loaded',
                'parameters': sum(p.numel() for p in model.parameters()),
                'device': next(model.parameters()).device.type,
                'metadata': self.model_metadata.get(model_name, {}),
                'layers': len(list(model.children()))
            }
        
        return performance_data
    
    def optimize_model_for_inference(self, model: nn.Module) -> nn.Module:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"""
        try:
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            model.eval()
            
            # ØªØ¹Ø·ÙŠÙ„ gradient Ù„Ø­ÙØ¸ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            for param in model.parameters():
                param.requires_grad = False
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            if hasattr(torch, 'compile') and torch.cuda.is_available():
                model = torch.compile(model, mode='reduce-overhead')
                st.info("âœ… Model optimized with torch.compile")
            
            return model
            
        except Exception as e:
            st.warning(f"âš ï¸ Model optimization skipped: {str(e)}")
            return model
    
    def save_model_checkpoint(self, model: nn.Module, metrics: Dict, model_name: str = 'hybrid'):
        """Ø­ÙØ¸ Ù†Ù‚Ø·Ø© ÙØ­Øµ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'metadata': {
                    'save_date': datetime.now().isoformat(),
                    'model_version': '2.1.0',
                    'performance_metrics': metrics,
                    'model_architecture': str(model.__class__.__name__)
                },
                'training_config': {
                    'optimizer': 'AdamW',
                    'learning_rate': 1e-4,
                    'epochs': 50
                }
            }
            
            save_path = self.model_paths.get(model_name, f'model_{model_name}.pth')
            torch.save(checkpoint, save_path)
            
            st.success(f"âœ… Model checkpoint saved: {save_path}")
            self._log_model_activity(model_name, 'saved')
            
        except Exception as e:
            st.error(f"âŒ Model save failed: {str(e)}")
    
    def model_size_analysis(self, model: nn.Module) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©"""
        param_size = 0
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        buffer_size = 0
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_all_mb = (param_size + buffer_size) / 1024**2
        
        return {
            'parameters_count': sum(p.numel() for p in model.parameters()),
            'total_size_mb': round(size_all_mb, 2),
            'layers_count': len(list(model.modules())),
            'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad)
        }
    
    def validate_model_compatibility(self, model: nn.Module, input_shape: tuple) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„"""
        try:
            # Ø§Ø®ØªØ¨Ø§Ø± ØªÙ…Ø±ÙŠØ± Ø¹ÙŠÙ†Ø©
            test_input = torch.randn(1, 3, *input_shape)
            with torch.no_grad():
                output = model(test_input)
            
            st.success(f"âœ… Model compatibility verified - Output shape: {output['final_prediction'].shape}")
            return True
            
        except Exception as e:
            st.error(f"âŒ Model compatibility check failed: {str(e)}")
            return False
    
    def _log_model_activity(self, model_name: str, action: str):
        """ØªØ³Ø¬ÙŠÙ„ Ù†Ø´Ø§Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'model': model_name,
            'action': action,
            'metadata': self.model_metadata.get(model_name, {})
        }
        
        # Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù log
        log_file = os.path.join(self.models_dir, 'model_activity.log')
        with open(log_file, 'a') as f:
            f.write(json.dumps(log_entry) + '\n')

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
@st.cache_resource
def get_model_manager():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
    return AdvancedModelManager()

# Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªÙ‚Ø¯Ù…
def demonstrate_model_capabilities():
    """Ø¹Ø±Ø¶ Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    manager = AdvancedModelManager()
    
    st.header("ğŸ§  Advanced Model Management")
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = manager.load_hybrid_model()
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
    performance = manager.get_model_performance()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Model Parameters", f"{performance['hybrid']['parameters']:,}")
    
    with col2:
        size_info = manager.model_size_analysis(model)
        st.metric("Model Size", f"{size_info['total_size_mb']} MB")
    
    with col3:
        st.metric("Model Status", performance['hybrid']['metadata'].get('status', 'Active'))
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    optimized_model = manager.optimize_model_for_inference(model)
    
    return optimized_model

if __name__ == "__main__":
    model = demonstrate_model_capabilities()
    st.success("âœ… Advanced Model Manager is ready!")
